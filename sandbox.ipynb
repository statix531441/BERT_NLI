{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# try:\n",
    "#     os.chdir('workspace')\n",
    "\n",
    "from options import Options\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>captionID</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>4609020271.jpg#3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4609020271.jpg#3r1n</td>\n",
       "      <td>A kid bored in a train with brown hair and his...</td>\n",
       "      <td>( ( A kid ) ( ( bored ( in ( ( ( ( a train ) (...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN kid)) (VP (VBD bored) ...</td>\n",
       "      <td>A child is riding the train from New York to B...</td>\n",
       "      <td>( ( A child ) ( ( is ( ( riding ( the train ) ...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN child)) (VP (VBZ is) (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
       "      <td>2313230479.jpg#2</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>2313230479.jpg#2r1c</td>\n",
       "      <td>A brown dog and black and white dog run along ...</td>\n",
       "      <td>( ( ( ( A ( brown dog ) ) and ) ( ( ( black an...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (JJ brown) (NN dog)) (...</td>\n",
       "      <td>Two squirrels run after acorns in the grass.</td>\n",
       "      <td>( ( Two squirrels ) ( ( run ( after ( acorns (...</td>\n",
       "      <td>(ROOT (S (NP (CD Two) (NNS squirrels)) (VP (VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[neutral, contradiction, contradiction, neutra...</td>\n",
       "      <td>3729748378.jpg#1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3729748378.jpg#1r1n</td>\n",
       "      <td>A person in full astronaut suit and gear train...</td>\n",
       "      <td>( ( ( A person ) ( in ( ( full ( astronaut ( s...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN i...</td>\n",
       "      <td>A female astronaut adjusting to the feeling of...</td>\n",
       "      <td>( ( ( A ( female astronaut ) ) ( ( ( adjusting...</td>\n",
       "      <td>(ROOT (NP (NP (DT A) (JJ female) (NN astronaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[entailment, entailment, contradiction, contra...</td>\n",
       "      <td>4708658738.jpg#0</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>4708658738.jpg#0r1e</td>\n",
       "      <td>An old Indian man dressed in rags sleeps on th...</td>\n",
       "      <td>( ( ( An ( old ( Indian man ) ) ) ( dressed ( ...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT An) (JJ old) (JJ Indian) ...</td>\n",
       "      <td>An old Indian man is dressed up</td>\n",
       "      <td>( ( An ( old ( Indian man ) ) ) ( is ( dressed...</td>\n",
       "      <td>(ROOT (S (NP (DT An) (JJ old) (JJ Indian) (NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
       "      <td>2372820502.jpg#0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2372820502.jpg#0r1n</td>\n",
       "      <td>A bunch of people are standing all together in...</td>\n",
       "      <td>( ( ( A bunch ) ( of people ) ) ( ( are ( ( st...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN bunch)) (PP (IN of...</td>\n",
       "      <td>A group of people are planning something.</td>\n",
       "      <td>( ( ( A group ) ( of people ) ) ( ( are ( plan...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN group)) (PP (IN of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    annotator_labels         captionID  \\\n",
       "0      [neutral, neutral, neutral, neutral, neutral]  4609020271.jpg#3   \n",
       "1  [contradiction, contradiction, contradiction, ...  2313230479.jpg#2   \n",
       "2  [neutral, contradiction, contradiction, neutra...  3729748378.jpg#1   \n",
       "3  [entailment, entailment, contradiction, contra...  4708658738.jpg#0   \n",
       "4      [neutral, neutral, neutral, neutral, neutral]  2372820502.jpg#0   \n",
       "\n",
       "      gold_label               pairID  \\\n",
       "0        neutral  4609020271.jpg#3r1n   \n",
       "1  contradiction  2313230479.jpg#2r1c   \n",
       "2        neutral  3729748378.jpg#1r1n   \n",
       "3  contradiction  4708658738.jpg#0r1e   \n",
       "4        neutral  2372820502.jpg#0r1n   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  A kid bored in a train with brown hair and his...   \n",
       "1  A brown dog and black and white dog run along ...   \n",
       "2  A person in full astronaut suit and gear train...   \n",
       "3  An old Indian man dressed in rags sleeps on th...   \n",
       "4  A bunch of people are standing all together in...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( A kid ) ( ( bored ( in ( ( ( ( a train ) (...   \n",
       "1  ( ( ( ( A ( brown dog ) ) and ) ( ( ( black an...   \n",
       "2  ( ( ( A person ) ( in ( ( full ( astronaut ( s...   \n",
       "3  ( ( ( An ( old ( Indian man ) ) ) ( dressed ( ...   \n",
       "4  ( ( ( A bunch ) ( of people ) ) ( ( are ( ( st...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (DT A) (NN kid)) (VP (VBD bored) ...   \n",
       "1  (ROOT (S (NP (NP (DT A) (JJ brown) (NN dog)) (...   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN i...   \n",
       "3  (ROOT (S (NP (NP (DT An) (JJ old) (JJ Indian) ...   \n",
       "4  (ROOT (S (NP (NP (DT A) (NN bunch)) (PP (IN of...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  A child is riding the train from New York to B...   \n",
       "1       Two squirrels run after acorns in the grass.   \n",
       "2  A female astronaut adjusting to the feeling of...   \n",
       "3                    An old Indian man is dressed up   \n",
       "4          A group of people are planning something.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( A child ) ( ( is ( ( riding ( the train ) ...   \n",
       "1  ( ( Two squirrels ) ( ( run ( after ( acorns (...   \n",
       "2  ( ( ( A ( female astronaut ) ) ( ( ( adjusting...   \n",
       "3  ( ( An ( old ( Indian man ) ) ) ( is ( dressed...   \n",
       "4  ( ( ( A group ) ( of people ) ) ( ( are ( plan...   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (DT A) (NN child)) (VP (VBZ is) (...  \n",
       "1  (ROOT (S (NP (CD Two) (NNS squirrels)) (VP (VB...  \n",
       "2  (ROOT (NP (NP (DT A) (JJ female) (NN astronaut...  \n",
       "3  (ROOT (S (NP (DT An) (JJ old) (JJ Indian) (NN ...  \n",
       "4  (ROOT (S (NP (NP (DT A) (NN group)) (PP (IN of...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('original/snli_1.0_test.jsonl', lines=True)\n",
    "df = df.sample(5, random_state=42).reset_index(drop=True)\n",
    "train_df = df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " 'a': 2,\n",
       " 'kid': 3,\n",
       " 'bored': 4,\n",
       " 'in': 5,\n",
       " 'train': 6,\n",
       " 'with': 7,\n",
       " 'brown': 8,\n",
       " 'hair': 9,\n",
       " 'and': 10,\n",
       " 'his': 11,\n",
       " 'head': 12,\n",
       " 'lying': 13,\n",
       " 'down': 14,\n",
       " 'dog': 15,\n",
       " 'black': 16,\n",
       " 'white': 17,\n",
       " 'run': 18,\n",
       " 'along': 19,\n",
       " 'the': 20,\n",
       " 'green': 21,\n",
       " 'grass': 22,\n",
       " 'person': 23,\n",
       " 'full': 24,\n",
       " 'astronaut': 25,\n",
       " 'suit': 26,\n",
       " 'gear': 27,\n",
       " 'trains': 28,\n",
       " 'an': 29,\n",
       " 'underwater': 30,\n",
       " 'lab': 31,\n",
       " 'as': 32,\n",
       " 'scuba': 33,\n",
       " 'diver': 34,\n",
       " 'assists': 35,\n",
       " 'old': 36,\n",
       " 'indian': 37,\n",
       " 'man': 38,\n",
       " 'dressed': 39,\n",
       " 'rags': 40,\n",
       " 'sleeps': 41,\n",
       " 'on': 42,\n",
       " 'ground': 43,\n",
       " 'using': 44,\n",
       " 'backpack': 45,\n",
       " 'pillow': 46,\n",
       " 'bunch': 47,\n",
       " 'of': 48,\n",
       " 'people': 49,\n",
       " 'are': 50,\n",
       " 'standing': 51,\n",
       " 'all': 52,\n",
       " 'together': 53,\n",
       " 'street': 54,\n",
       " 'building': 55,\n",
       " 'background': 56,\n",
       " 'child': 57,\n",
       " 'is': 58,\n",
       " 'riding': 59,\n",
       " 'from': 60,\n",
       " 'new': 61,\n",
       " 'york': 62,\n",
       " 'to': 63,\n",
       " 'boston': 64,\n",
       " 'two': 65,\n",
       " 'squirrels': 66,\n",
       " 'after': 67,\n",
       " 'acorns': 68,\n",
       " 'female': 69,\n",
       " 'adjusting': 70,\n",
       " 'feeling': 71,\n",
       " 'low': 72,\n",
       " 'gravity': 73,\n",
       " 'by': 74,\n",
       " 'training': 75,\n",
       " 'up': 76,\n",
       " 'group': 77,\n",
       " 'planning': 78,\n",
       " 'something': 79}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "\n",
    "## Tad bit weird so just change train.py\n",
    "opt.model = \"LSTM\"\n",
    "opt.model_folder = \"models/LSTM_TEST\"\n",
    "opt.data_folder = \"data/LSTM_TEST\"\n",
    "os.makedirs(opt.model_folder, exist_ok=True)\n",
    "os.makedirs(opt.data_folder, exist_ok=True)\n",
    "\n",
    "opt.embedding_dim = 6\n",
    "opt.hidden_dim = 4\n",
    "\n",
    "vocab = create_vocab(train_df, opt)\n",
    "vocab_size = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.save_options(opt.model_folder)\n",
    "\n",
    "word_embeddings = nn.Embedding(vocab_size, opt.embedding_dim)\n",
    "\n",
    "# Replace this line with actual fasttext embeddings\n",
    "pretrained_fasttext_embeddings  = torch.rand((vocab_size, opt.embedding_dim))\n",
    "\n",
    "word_embeddings.weight.data.copy_(pretrained_fasttext_embeddings)\n",
    "torch.save(word_embeddings.state_dict(), f'{opt.data_folder}/word_embeddings.pth')\n",
    "\n",
    "\n",
    "## Init\n",
    "vocab = load_vocab(opt)\n",
    "word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n",
    "word_embeddings.load_state_dict(torch.load(f'{opt.data_folder}/word_embeddings.pth'))\n",
    "\n",
    "\n",
    "# Indexing\n",
    "idx = 0\n",
    "sentence1, sentence2 = df.loc[idx, ['sentence1', 'sentence2']]\n",
    "sentence1, sentence2 = clean_text(sentence1), clean_text(sentence2)\n",
    "\n",
    "y = torch.tensor(le(df.loc[idx, 'gold_label']))\n",
    "\n",
    "input_ids1 = torch.ones(100, dtype=torch.long)\n",
    "input_ids2 = torch.ones(100, dtype=torch.long)\n",
    "\n",
    "for i, word in enumerate(sentence1.split()):\n",
    "    input_ids1[i] = vocab[word] if word in vocab else 0\n",
    "\n",
    "for i, word in enumerate(sentence2.split()):\n",
    "    input_ids2[i] = vocab[word] if word in vocab else 0\n",
    "\n",
    "embeds1, embeds2 = word_embeddings(input_ids1), word_embeddings(input_ids2)\n",
    "X = [embeds1, embeds2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "## Init\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "sentence1 = df['sentence1'].tolist()\n",
    "sentence2 = df['sentence2'].tolist()\n",
    "info = tokenizer(sentence1, sentence2, padding='max_length', max_length=200, return_tensors='pt')\n",
    "\n",
    "input_ids = info['input_ids']\n",
    "attention_mask = info['attention_mask']\n",
    "\n",
    "y = torch.tensor(df['gold_label'].apply(lambda label: le(label)))\n",
    "\n",
    "## Get idx\n",
    "idx = 0\n",
    "input_ids = input_ids[idx]\n",
    "attention_mask = attention_mask[idx]\n",
    "\n",
    "X = [input_ids, attention_mask]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\statix/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "c:\\Users\\statix\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "\n",
    "## Init\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "sentence1 = df['sentence1'].tolist()\n",
    "sentence2 = df['sentence2'].tolist()\n",
    "info = tokenizer(sentence1, sentence2, padding='max_length', max_length=200, return_tensors='pt')\n",
    "\n",
    "input_ids = info['input_ids']\n",
    "token_type_ids = info['token_type_ids']\n",
    "attention_mask = info['attention_mask']\n",
    "\n",
    "y = torch.tensor(df['gold_label'].apply(lambda label: le(label)))\n",
    "\n",
    "\n",
    "## Get idx\n",
    "idx = 0\n",
    "input_ids = input_ids[idx]\n",
    "token_type_ids = token_type_ids[idx]\n",
    "attention_mask = attention_mask[idx]\n",
    "\n",
    "X = [input_ids, token_type_ids, attention_mask]\n",
    "y = y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x.view(1,*x.shape) for x in X]\n",
    "y = y.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [torch.randn(30,*x.shape) for x in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim, batch_first=True, bidirectional=True)\n",
    "embeds1, embeds2 = X\n",
    "\n",
    "out1, (h, c) = lstm(embeds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward and backward concatenated\n",
    "out1[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward and backward not concatenated\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0512,  0.0485, -0.2428,  0.0412,  0.1431,  0.1300, -0.1976, -0.0655],\n",
       "        [-0.2403,  0.1184, -0.3530,  0.2882,  0.0499, -0.0162, -0.0347, -0.1560],\n",
       "        [-0.1178, -0.0512, -0.3399,  0.1470, -0.0422,  0.0048,  0.0779, -0.1458],\n",
       "        [-0.0479,  0.1407, -0.0342, -0.0346,  0.0566,  0.0123, -0.2221,  0.0120],\n",
       "        [-0.2458,  0.1486, -0.1870,  0.1065,  0.0017, -0.1152, -0.2017, -0.2473],\n",
       "        [-0.1424, -0.0145, -0.2297, -0.1777,  0.0283,  0.0373, -0.0799, -0.1652],\n",
       "        [-0.0780,  0.1414, -0.0921, -0.2335,  0.0792,  0.0506,  0.0436, -0.1143],\n",
       "        [-0.0976, -0.0558,  0.2510, -0.2692,  0.0132, -0.0167,  0.0125, -0.2026],\n",
       "        [-0.0247,  0.0237, -0.2977,  0.0070,  0.0519,  0.0352, -0.0099, -0.1840],\n",
       "        [-0.0364,  0.0998, -0.2174,  0.1123,  0.0827,  0.1107, -0.3111,  0.1549],\n",
       "        [-0.1475,  0.1525, -0.2729,  0.1515,  0.0743, -0.0374, -0.1184, -0.1717],\n",
       "        [-0.0825,  0.5651, -0.2147,  0.0653,  0.0875, -0.0460,  0.0120, -0.2157],\n",
       "        [ 0.0415,  0.2617,  0.0178,  0.0119,  0.0806, -0.0728, -0.0781, -0.2479],\n",
       "        [-0.0678,  0.1466, -0.0872,  0.0009,  0.1128, -0.0884, -0.1228, -0.1583],\n",
       "        [-0.0539,  0.0638, -0.3189,  0.1175,  0.0290,  0.0341, -0.1013, -0.2492],\n",
       "        [-0.2384,  0.0619, -0.1704, -0.2634,  0.0448,  0.0340,  0.0355, -0.2318],\n",
       "        [-0.1143,  0.1827, -0.2164,  0.1897,  0.1308,  0.0348, -0.1750,  0.1181],\n",
       "        [ 0.0263,  0.3750, -0.1433,  0.0508,  0.0808, -0.0860,  0.0447, -0.3069],\n",
       "        [-0.1767,  0.0225, -0.2979,  0.0068,  0.0400,  0.0274, -0.1371, -0.2644],\n",
       "        [-0.0843,  0.0735,  0.0149,  0.0705,  0.0683,  0.0988, -0.2928, -0.0424],\n",
       "        [-0.2577, -0.0941, -0.3801,  0.0260, -0.0557, -0.0143,  0.0627, -0.1603],\n",
       "        [-0.2911,  0.0223, -0.2814,  0.3869,  0.0288, -0.0892, -0.0527, -0.0502],\n",
       "        [-0.3015,  0.0695, -0.5164, -0.0655, -0.0979,  0.0014,  0.0975, -0.2659],\n",
       "        [-0.2606,  0.1330,  0.0168, -0.2089, -0.0341,  0.0014,  0.1243, -0.2449],\n",
       "        [-0.1796,  0.3120,  0.2314,  0.1072,  0.0508, -0.0799, -0.1293, -0.0306],\n",
       "        [-0.0082,  0.1084,  0.3904,  0.0733,  0.0040, -0.1150, -0.3038, -0.3056],\n",
       "        [ 0.1292,  0.1819, -0.2703,  0.1309,  0.1356,  0.0592, -0.1862, -0.0700],\n",
       "        [-0.1236,  0.3538,  0.0240,  0.0078,  0.0695, -0.0472, -0.1716, -0.1240],\n",
       "        [ 0.1822,  0.2065, -0.1030,  0.1940,  0.1188, -0.0027, -0.2233,  0.3806],\n",
       "        [ 0.1059,  0.2634, -0.0492, -0.1512,  0.0286,  0.0523,  0.0732, -0.3363]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the last vector as sentence representation\n",
    "out1[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\statix/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1687,  1.4565, -0.5958]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "\n",
    "\n",
    "bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "linear = nn.Linear(768, 3)\n",
    "\n",
    "input_ids, token_type_ids, attention_mask = X\n",
    "\n",
    "out = bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\n",
    "out = out[1] # CLS representation of both sentences\n",
    "out = linear(out)  # Cross-Entropy Loss takes in the unnormalized logits\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
